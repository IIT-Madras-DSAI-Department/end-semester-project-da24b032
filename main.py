# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WYwpgoJ4mJT80HI7ynZOfwUfo7acqDk_
"""

from google.colab import files

uploaded = files.upload()

from google.colab import files

uploaded = files.upload()

import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from collections import Counter
import os

# --- Project Setup ---

# 1. Set Random Seeds for Reproducibility
SEED = 42
np.random.seed(SEED)

# Placeholder for timing utility
def timer_start():
    return time.time()

def timer_end(start_time, message="Operation"):
    end_time = time.time()
    duration = end_time - start_time
    print(f"{message} took {duration:.4f} seconds.")
    return duration

print("Setup complete. Random seed set to 42.")

# --- Load MNIST train & validation CSVs ---
# Assuming files are accessible via the provided names

try:
    # Load dataframes
    df_train = pd.read_csv("MNIST_train.csv")
    df_val = pd.read_csv("MNIST_validation.csv")

    # Separate X (features) and y (labels)
    # The first column is 'label'
    y_train = df_train['label'].values.reshape(-1, 1)
    X_train = df_train.drop('label', axis=1).values

    y_val = df_val['label'].values.reshape(-1, 1)
    X_val = df_val.drop('label', axis=1).values

    print(f"Original Training Data Shape: X={X_train.shape}, y={y_train.shape}")
    print(f"Original Validation Data Shape: X={X_val.shape}, y={y_val.shape}")

except FileNotFoundError:
    print("Error: MNIST CSV files not found.")
    raise

# --- Preprocess data ---

# Normalize pixels (0-1)
X_train_norm = X_train / 255.0
X_val_norm = X_val / 255.0

# Check shapes and label balance
print("\n--- Data Preprocessing & Check ---")
print(f"Normalized Training Features Shape: {X_train_norm.shape}")
print(f"Normalized Validation Features Shape: {X_val_norm.shape}")

# Check label balance (Top 3 most common labels)
train_label_counts = Counter(y_train.flatten())
val_label_counts = Counter(y_val.flatten())
print(f"Training Label Counts (Top 3): {train_label_counts.most_common(3)}")
print(f"Validation Label Counts (Top 3): {val_label_counts.most_common(3)}")
print("Data loading and normalization complete.")

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix as sk_confusion_matrix, accuracy_score as sk_accuracy_score, precision_recall_fscore_support

# --- Implicit Fix for Feature Count ---
# If the normalized features are 785 columns, we assume the first column (index 0)
# is the residual label and slice it out to keep the correct 784 pixel features.

if X_train_norm.shape[1] == 785:
    print("\n[INFO] Correcting feature matrix shape: Dropping assumed residual column (index 0).")
    X_train_norm = X_train_norm[:, 1:]
    X_val_norm = X_val_norm[:, 1:]

print(f"Final Normalized Training Features Shape: {X_train_norm.shape}")
print(f"Final Normalized Validation Features Shape: {X_val_norm.shape}")

# --- Create a “dev subset” for tuning ---

# Using the first 3000 samples for the train_dev set
N_TRAIN_DEV = 3000
X_train_dev = X_train_norm[:N_TRAIN_DEV, :]
y_train_dev = y_train[:N_TRAIN_DEV]

# Using the first 1000 samples from the full validation set for val_dev
N_VAL_DEV = 1000
X_val_dev = X_val_norm[:N_VAL_DEV, :]
y_val_dev = y_val[:N_VAL_DEV]

print("\n--- Development Subset Created ---")
print(f"X_train_dev shape: {X_train_dev.shape}, y_train_dev shape: {y_train_dev.shape}")
print(f"X_val_dev shape: {X_val_dev.shape}, y_val_dev shape: {y_val_dev.shape}")


# --- Common evaluation utilities (from scratch using NumPy) ---

def accuracy_score(y_true, y_pred):
    """Computes the classification accuracy."""
    return np.mean(y_true.flatten() == y_pred.flatten())

def confusion_matrix(y_true, y_pred, n_classes=10):
    """Computes the confusion matrix."""
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()
    matrix = np.zeros((n_classes, n_classes), dtype=np.int64)
    for i in range(len(y_true)):
        matrix[y_true[i], y_pred[i]] += 1
    return matrix

def precision_recall_f1_macro(y_true, y_pred, n_classes=10):
    """Computes precision, recall, and F1 score for each class, and the macro F1."""

    y_true = y_true.flatten().astype(np.int32)
    y_pred = y_pred.flatten().astype(np.int32)

    cm = confusion_matrix(y_true, y_pred, n_classes)
    metrics = []

    for c in range(n_classes):
        TP = cm[c, c]
        FP = np.sum(cm[:, c]) - TP
        FN = np.sum(cm[c, :]) - TP

        # Precision, Recall, F1 (Handle division by zero)
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        metrics.append({
            'class': c,
            'precision': precision,
            'recall': recall,
            'f1': f1
        })

    macro_f1 = np.mean([m['f1'] for m in metrics])

    return {
        'macro_f1': macro_f1,
        'per_class': metrics,
        'confusion_matrix': cm
    }

def print_metrics_table(results_dict, set_name="Dev Set"):
    """Helper to tabulate and print results."""
    print(f"\n--- {set_name} Results ---")
    print(f"Accuracy: {results_dict['accuracy']:.4f}")
    print(f"Macro F1 Score: {results_dict['macro_f1']:.4f}")

    print("\n| Class | Precision | Recall | F1 Score |")
    print("|---|---|---|---|")
    # Assuming the results_dict contains 'per_class_metrics' from precision_recall_f1_macro
    for m in results_dict['per_class_metrics']['per_class']:
        print(f"| {m['class']:<5} | {m['precision']:.4f} | {m['recall']:.4f} | {m['f1']:.4f} |")

print("\nEvaluation utilities implemented: accuracy_score, confusion_matrix, precision_recall_f1_macro.")
print("Proceeding to Multinomial Logistic Regression (Softmax) implementation.")

# --- Helper for One-Hot Encoding (OHE) ---
def one_hot_encode(y, n_classes=10):
    """Converts a column vector of labels (N, 1) to OHE matrix (N, K)."""
    y = y.flatten()
    N = y.shape[0]
    Y_OHE = np.zeros((N, n_classes))
    Y_OHE[np.arange(N), y] = 1
    return Y_OHE

# --- Helper for Data Augmentation (Bias) ---
def augment_X(X):
    """Augments the feature matrix X with a column of ones for the bias term."""
    return np.hstack([np.ones((X.shape[0], 1)), X])

class SoftmaxClassifier:

    def __init__(self, n_classes=10, learning_rate=0.1, lambda_reg=0.01, n_epochs=10, batch_size=128):
        self.n_classes = n_classes
        self.learning_rate = learning_rate
        self.lambda_reg = lambda_reg
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.W = None # Weights matrix (D_aug, K)

    def _softmax(self, Z):
        # Subtract max(Z) for numerical stability
        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)

    def _cross_entropy_loss(self, P, Y_OHE, W, lambda_reg):
        N = P.shape[0]
        # Cross-Entropy Loss
        loss = -np.sum(Y_OHE * np.log(P + 1e-9)) / N

        # L2 Regularization (exclude bias W[0,:])
        l2_penalty = (lambda_reg / 2) * np.sum(W[1:, :]**2)

        return loss + l2_penalty

    def fit(self, X, y):
        # 0. Setup
        X_aug = augment_X(X)
        Y_OHE = one_hot_encode(y, self.n_classes)
        N, D_aug = X_aug.shape
        K = self.n_classes

        # Initialize weights
        self.W = np.random.randn(D_aug, K) * 0.01
        cost_history = []

        start_time = timer_start()
        print(f"--- Starting Softmax Training (Epochs: {self.n_epochs}, LR: {self.learning_rate}, Lambda: {self.lambda_reg}) ---")

        for epoch in range(1, self.n_epochs + 1):

            # 1. Shuffle and Mini-batch setup
            permutation = np.random.permutation(N)
            X_shuffled = X_aug[permutation]
            Y_OHE_shuffled = Y_OHE[permutation]

            total_cost = 0

            for i in range(0, N, self.batch_size):
                X_batch = X_shuffled[i:i + self.batch_size]
                Y_OHE_batch = Y_OHE_shuffled[i:i + self.batch_size]
                N_batch = X_batch.shape[0]

                # 2. Forward Pass
                Z = X_batch @ self.W
                P = self._softmax(Z)

                # 3. Calculate Gradient
                E = P - Y_OHE_batch
                grad_W = X_batch.T @ E / N_batch

                # Add L2 Regularization gradient (on non-bias terms)
                grad_reg = self.W.copy()
                grad_reg[0, :] = 0
                grad_W += self.lambda_reg * grad_reg

                # 4. Update Weights
                self.W -= self.learning_rate * grad_W

                # 5. Track Cost
                total_cost += self._cross_entropy_loss(P, Y_OHE_batch, self.W, self.lambda_reg) * N_batch

            avg_cost = total_cost / N
            cost_history.append(avg_cost)

            if epoch % 5 == 0 or epoch == 1 or epoch == self.n_epochs:
                print(f"Epoch {epoch}/{self.n_epochs} - Avg. Training Cost: {avg_cost:.6f}")

        timer_end(start_time, "Softmax Training")
        return self.W, cost_history

    def predict_proba(self, X):
        X_aug = augment_X(X)
        Z = X_aug @ self.W
        return self._softmax(Z)

    def predict(self, X):
        # Returns the predicted class index (0-9)
        probabilities = self.predict_proba(X)
        return np.argmax(probabilities, axis=1).reshape(-1, 1)

# --- Initial Test Run (Softmax Tuning) ---
SOFTMAX_DEFAULTS = {
    'learning_rate': 0.1,
    'lambda_reg': 0.01,
    'n_epochs': 20,
    'batch_size': 128
}

print("\n--- Step 3: Multinomial Logistic Regression (Softmax) Implementation ---")
print("Implementation complete. Running initial test on dev subset.")

softmax_model = SoftmaxClassifier(**SOFTMAX_DEFAULTS)
W_test, cost_history_test = softmax_model.fit(X_train_dev, y_train_dev)

# Evaluate on X_val_dev
y_pred_dev = softmax_model.predict(X_val_dev)

# Calculate metrics
acc_dev = accuracy_score(y_val_dev, y_pred_dev)
metrics_dev = precision_recall_f1_macro(y_val_dev, y_pred_dev)

print("\n--- Initial Dev Validation Results ---")
print(f"Accuracy: {acc_dev:.4f}")
print(f"Macro F1: {metrics_dev['macro_f1']:.4f}")

print("\nProceeding to Hyperparameter Tuning (Step 4).")

# --- Step 4: Hyperparameter Tuning for Logistic Regression (Softmax) ---

# Hyperparameter Grid
LR_VALUES = [0.01, 0.05, 0.1]
LAMBDA_VALUES = [0.0001, 0.001, 0.01]
EPOCH_VALUES = [10, 20]
BATCH_VALUES = [64, 128]

tuning_results = []
best_f1 = -1.0
best_config = None

print("\n--- Starting Softmax Hyperparameter Grid Search ---")
print(f"Total configurations to test: {len(LR_VALUES) * len(LAMBDA_VALUES) * len(EPOCH_VALUES) * len(BATCH_VALUES)}")

# Start overall timer
tuning_start_time = timer_start()

for lr in LR_VALUES:
    for lmbd in LAMBDA_VALUES:
        for epoch in EPOCH_VALUES:
            for batch in BATCH_VALUES:

                print(f"\nTesting: LR={lr}, Lambda={lmbd}, Epochs={epoch}, Batch={batch}")

                # 1. Initialize and Train Model
                model = SoftmaxClassifier(
                    learning_rate=lr,
                    lambda_reg=lmbd,
                    n_epochs=epoch,
                    batch_size=batch
                )

                # Fit returns W and cost_history, which includes the timer output
                model.fit(X_train_dev, y_train_dev)

                # 2. Evaluate on Train Dev Set (for Bias/Variance analysis later)
                y_pred_train_dev = model.predict(X_train_dev)
                acc_train_dev = accuracy_score(y_train_dev, y_pred_train_dev)
                f1_train_dev = precision_recall_f1_macro(y_train_dev, y_pred_train_dev)['macro_f1']

                # 3. Evaluate on Validation Dev Set (for selection)
                y_pred_val_dev = model.predict(X_val_dev)
                acc_val_dev = accuracy_score(y_val_dev, y_pred_val_dev)
                f1_val_dev = precision_recall_f1_macro(y_val_dev, y_pred_val_dev)['macro_f1']

                print(f"  Train Dev F1: {f1_train_dev:.4f} | Val Dev F1: {f1_val_dev:.4f}")

                # 4. Store Results
                config_result = {
                    'LR': lr,
                    'Lambda': lmbd,
                    'Epochs': epoch,
                    'Batch': batch,
                    'Train_Acc': acc_train_dev,
                    'Train_F1': f1_train_dev,
                    'Val_Acc': acc_val_dev,
                    'Val_F1': f1_val_dev,
                }
                tuning_results.append(config_result)

                # 5. Check for Best Configuration
                if f1_val_dev > best_f1:
                    best_f1 = f1_val_dev
                    best_config = config_result

timer_end(tuning_start_time, "\nTotal Softmax Hyperparameter Tuning")

# Display Best Result
print("\n" + "="*50)
print("BEST SOFTMAX CONFIGURATION (Based on Val Dev Macro F1 Score):")
print(f"Macro F1 Score: {best_config['Val_F1']:.4f}")
print(f"Hyperparameters: LR={best_config['LR']}, Lambda={best_config['Lambda']}, Epochs={best_config['Epochs']}, Batch={best_config['Batch']}")
print("="*50)

# Save the full results table to a CSV for later analysis/reporting
tuning_df = pd.DataFrame(tuning_results)
tuning_df.to_csv("softmax_tuning_results.csv", index=False)
print("Full tuning results saved to 'softmax_tuning_results.csv'")

print("\nProceeding to Final Logistic Regression training & evaluation (Step 5).")

# --- Step 5: Final Logistic Regression training & evaluation ---

BEST_SOFTMAX_CONFIG = {
    'learning_rate': 0.1,
    'lambda_reg': 0.001,
    'n_epochs': 20,
    'batch_size': 64
}

print("\n--- Training Final Softmax Model on Full Data ---")
print(f"Hyperparameters: LR={BEST_SOFTMAX_CONFIG['learning_rate']}, Lambda={BEST_SOFTMAX_CONFIG['lambda_reg']}, Epochs={BEST_SOFTMAX_CONFIG['n_epochs']}, Batch={BEST_SOFTMAX_CONFIG['batch_size']}")

# 1. Initialize and Train Model on full training data
final_softmax_model = SoftmaxClassifier(**BEST_SOFTMAX_CONFIG)

# Record training time separately
start_time_full_train = timer_start()
final_softmax_model.fit(X_train_norm, y_train)
softmax_train_time = timer_end(start_time_full_train, "Final Softmax Training")


# 2. Evaluate on Full Validation Set
y_pred_val_softmax = final_softmax_model.predict(X_val_norm)

# Calculate metrics
acc_val_softmax = accuracy_score(y_val, y_pred_val_softmax)
metrics_val_softmax = precision_recall_f1_macro(y_val, y_pred_val_softmax)

# 3. Tabulate and Store Results
softmax_final_results = {
    'Model': 'Softmax (Logistic Regression)',
    'Train_Time': softmax_train_time,
    'Accuracy': acc_val_softmax,
    'Macro_F1': metrics_val_softmax['macro_f1'],
    'Confusion_Matrix': metrics_val_softmax['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_softmax
}

print("\n" + "="*60)
print("FINAL SOFTMAX REGRESSION (LOGISTIC) EVALUATION (Full Validation Set)")
print("="*60)
print(f"Validation Accuracy: {softmax_final_results['Accuracy']:.4f}")
print(f"Validation Macro F1 Score: {softmax_final_results['Macro_F1']:.4f}")
print(f"Training Time: {softmax_final_results['Train_Time']:.4f} seconds")

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': softmax_final_results['Accuracy'],
     'macro_f1': softmax_final_results['Macro_F1'],
     'per_class_metrics': softmax_final_results['Per_Class_Metrics']},
    set_name="Full Validation Set"
)

# Store the final model and results for the ensemble step
FINAL_MODELS = {'Softmax': final_softmax_model}
FINAL_RESULTS = [softmax_final_results]

print("\nProceeding to Linear Regression (One-vs-Rest) implementation & tuning (Step 6).")

class LinearRegressionOVR:

    def __init__(self, n_classes=10, learning_rate=0.01, lambda_reg=0.01, n_epochs=10, batch_size=128):
        self.n_classes = n_classes
        self.learning_rate = learning_rate
        self.lambda_reg = lambda_reg
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.W_list = [] # List to hold 10 weight vectors (D_aug, 1)
        self.W_stacked = None # Combined weight matrix (D_aug, K)

    def _mse_loss(self, y_true, y_pred, W, lambda_reg):
        N = y_true.shape[0]
        # Mean Squared Error
        mse = np.sum((y_pred - y_true)**2) / (2 * N)

        # L2 Regularization (Ridge) (exclude bias W[0])
        l2_penalty = (lambda_reg / 2) * np.sum(W[1:]**2)

        return mse + l2_penalty

    def _single_fit(self, X_aug, y_binary):
        N, D_aug = X_aug.shape

        # Initialize weights for this class
        W = np.random.randn(D_aug, 1) * 0.01

        # Training loop for a single regressor
        for epoch in range(self.n_epochs):
            permutation = np.random.permutation(N)
            X_shuffled = X_aug[permutation]
            y_shuffled = y_binary[permutation]

            for i in range(0, N, self.batch_size):
                X_batch = X_shuffled[i:i + self.batch_size]
                y_batch = y_shuffled[i:i + self.batch_size]
                N_batch = X_batch.shape[0]

                # Forward Pass: Predict score (Y = XW)
                y_pred = X_batch @ W

                # Error
                E = y_pred - y_batch

                # Gradient (excluding regularization)
                grad_W = X_batch.T @ E / N_batch

                # Add L2 Regularization gradient (on non-bias terms)
                grad_reg = W.copy()
                grad_reg[0] = 0 # No regularization on bias term
                grad_W += self.lambda_reg * grad_reg

                # Update Weights
                W -= self.learning_rate * grad_W

        return W

    def fit(self, X, y):
        self.W_list = []
        X_aug = augment_X(X)
        y = y.flatten()

        start_time = timer_start()
        print(f"--- Starting OVR Linear Regression Training (Epochs: {self.n_epochs}, LR: {self.learning_rate}, Lambda: {self.lambda_reg}) ---")

        # Train one regressor for each class (0 through 9)
        for c in range(self.n_classes):
            # Target is 1 if class is c, 0 otherwise
            y_binary = (y == c).astype(np.float64).reshape(-1, 1)

            W_c = self._single_fit(X_aug, y_binary)
            self.W_list.append(W_c)

        # Combine all 10 weight vectors into a single matrix (D_aug, K) for prediction efficiency
        self.W_stacked = np.hstack(self.W_list)

        timer_end(start_time, "OVR Training")
        return self.W_stacked

    def predict_scores(self, X):
        X_aug = augment_X(X)
        # Scores Z = X_aug @ W_stacked
        return X_aug @ self.W_stacked

    def predict(self, X):
        # Predict class by choosing the regressor with the highest score
        scores = self.predict_scores(X)
        return np.argmax(scores, axis=1).reshape(-1, 1)

# --- Initial Test Run (OVR Tuning) ---
OVR_DEFAULTS = {
    'learning_rate': 0.05,
    'lambda_reg': 0.01,
    'n_epochs': 20,
    'batch_size': 128
}

print("\n--- Step 6: Linear Regression (One-vs-Rest) Implementation ---")
print("Implementation complete. Running initial test on dev subset.")

ovr_model_test = LinearRegressionOVR(**OVR_DEFAULTS)
W_ovr_test = ovr_model_test.fit(X_train_dev, y_train_dev)

# Evaluate on X_val_dev
y_pred_ovr_dev = ovr_model_test.predict(X_val_dev)

# Calculate metrics
acc_ovr_dev = accuracy_score(y_val_dev, y_pred_ovr_dev)
metrics_ovr_dev = precision_recall_f1_macro(y_val_dev, y_pred_ovr_dev)

print("\n--- Initial OVR Dev Validation Results ---")
print(f"Accuracy: {acc_ovr_dev:.4f}")
print(f"Macro F1: {metrics_ovr_dev['macro_f1']:.4f}")

print("\nProceeding to OVR Hyperparameter Tuning (Step 6 continued).")

# --- Step 6 Continued: Hyperparameter Tuning for OVR Linear Regression ---

# Hyperparameter Grid
LR_VALUES_OVR = [0.01, 0.05, 0.1]
LAMBDA_VALUES_OVR = [0.0001, 0.001, 0.01]
EPOCH_VALUES_OVR = [10, 20]
BATCH_VALUES_OVR = [64, 128]

ovr_tuning_results = []
best_ovr_f1 = -1.0
best_ovr_config = None

print("\n--- Starting OVR Linear Regression Hyperparameter Grid Search ---")
total_configs = len(LR_VALUES_OVR) * len(LAMBDA_VALUES_OVR) * len(EPOCH_VALUES_OVR) * len(BATCH_VALUES_OVR)
print(f"Total configurations to test: {total_configs}")

# Start overall timer
ovr_tuning_start_time = timer_start()

for lr in LR_VALUES_OVR:
    for lmbd in LAMBDA_VALUES_OVR:
        for epoch in EPOCH_VALUES_OVR:
            for batch in BATCH_VALUES_OVR:

                print(f"\nTesting: LR={lr}, Lambda={lmbd}, Epochs={epoch}, Batch={batch}")

                # 1. Initialize and Train Model
                model = LinearRegressionOVR(
                    learning_rate=lr,
                    lambda_reg=lmbd,
                    n_epochs=epoch,
                    batch_size=batch
                )

                model.fit(X_train_dev, y_train_dev)

                # 2. Evaluate on Train Dev Set (for Bias/Variance analysis)
                y_pred_train_dev = model.predict(X_train_dev)
                acc_train_dev = accuracy_score(y_train_dev, y_pred_train_dev)
                f1_train_dev = precision_recall_f1_macro(y_train_dev, y_pred_train_dev)['macro_f1']

                # 3. Evaluate on Validation Dev Set (for selection)
                y_pred_val_dev = model.predict(X_val_dev)
                acc_val_dev = accuracy_score(y_val_dev, y_pred_val_dev)
                f1_val_dev = precision_recall_f1_macro(y_val_dev, y_pred_val_dev)['macro_f1']

                print(f"  Train Dev F1: {f1_train_dev:.4f} | Val Dev F1: {f1_val_dev:.4f}")

                # 4. Store Results
                config_result = {
                    'LR': lr,
                    'Lambda': lmbd,
                    'Epochs': epoch,
                    'Batch': batch,
                    'Train_Acc': acc_train_dev,
                    'Train_F1': f1_train_dev,
                    'Val_Acc': acc_val_dev,
                    'Val_F1': f1_val_dev,
                }
                ovr_tuning_results.append(config_result)

                # 5. Check for Best Configuration
                if f1_val_dev > best_ovr_f1:
                    best_ovr_f1 = f1_val_dev
                    best_ovr_config = config_result

timer_end(ovr_tuning_start_time, "\nTotal OVR Hyperparameter Tuning")

# Display Best Result
print("\n" + "="*60)
print("BEST OVR LINEAR REGRESSION CONFIGURATION (Based on Val Dev Macro F1 Score):")
print(f"Macro F1 Score: {best_ovr_config['Val_F1']:.4f}")
print(f"Hyperparameters: LR={best_ovr_config['LR']}, Lambda={best_ovr_config['Lambda']}, Epochs={best_ovr_config['Epochs']}, Batch={best_ovr_config['Batch']}")
print("="*60)

# Save the full results table to a CSV for later analysis/reporting
ovr_tuning_df = pd.DataFrame(ovr_tuning_results)
ovr_tuning_df.to_csv("ovr_tuning_results.csv", index=False)
print("Full tuning results saved to 'ovr_tuning_results.csv'")

print("\nProceeding to Final OVR Linear Regression training & evaluation (Step 7).")

# --- Step 7: Final OVR Linear Regression training & evaluation ---

BEST_OVR_CONFIG = {
    'learning_rate': 0.01,
    'lambda_reg': 0.0001,
    'n_epochs': 10,
    'batch_size': 64
}

print("\n--- Training Final OVR Linear Regression Model on Full Data ---")
print(f"Hyperparameters: LR={BEST_OVR_CONFIG['learning_rate']}, Lambda={BEST_OVR_CONFIG['lambda_reg']}, Epochs={BEST_OVR_CONFIG['n_epochs']}, Batch={BEST_OVR_CONFIG['batch_size']}")

# 1. Initialize and Train Model on full training data
final_ovr_model = LinearRegressionOVR(**BEST_OVR_CONFIG)

# Record training time separately
start_time_full_train = timer_start()
final_ovr_model.fit(X_train_norm, y_train)
ovr_train_time = timer_end(start_time_full_train, "Final OVR Training")


# 2. Evaluate on Full Validation Set
y_pred_val_ovr = final_ovr_model.predict(X_val_norm)

# Calculate metrics
acc_val_ovr = accuracy_score(y_val, y_pred_val_ovr)
metrics_val_ovr = precision_recall_f1_macro(y_val, y_pred_val_ovr)

# 3. Tabulate and Store Results
ovr_final_results = {
    'Model': 'Linear Regression (OVR)',
    'Train_Time': ovr_train_time,
    'Accuracy': acc_val_ovr,
    'Macro_F1': metrics_val_ovr['macro_f1'],
    'Confusion_Matrix': metrics_val_ovr['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_ovr
}

# Add results and model to the existing global storage
FINAL_MODELS['OVR'] = final_ovr_model
FINAL_RESULTS.append(ovr_final_results)

print("\n" + "="*60)
print("FINAL OVR LINEAR REGRESSION EVALUATION (Full Validation Set)")
print("="*60)
print(f"Validation Accuracy: {ovr_final_results['Accuracy']:.4f}")
print(f"Validation Macro F1 Score: {ovr_final_results['Macro_F1']:.4f}")
print(f"Training Time: {ovr_final_results['Train_Time']:.4f} seconds")

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': ovr_final_results['Accuracy'],
     'macro_f1': ovr_final_results['Macro_F1'],
     'per_class_metrics': ovr_final_results['Per_Class_Metrics']},
    set_name="Full Validation Set"
)

print("\nProceeding to Ensemble (Softmax + OVR) Implementation (Step 8).")

# --- Step 8: Ensemble (Softmax + OVR) Implementation and Evaluation ---

class EnsembleClassifier:
    """Ensembles multiple classifiers by averaging their output scores."""

    def __init__(self, models):
        """
        models: A dictionary of trained models, e.g., {'Softmax': SoftmaxModel, 'OVR': OVRModel}.
        """
        self.models = models

    def predict_scores(self, X):
        """Calculates the average score across all models."""

        # 1. Get scores from Softmax (Probabilities)
        softmax_scores = self.models['Softmax'].predict_proba(X)

        # 2. Get scores from OVR (Raw Output Scores)
        ovr_scores = self.models['OVR'].predict_scores(X)

        # 3. Combine scores
        # Note: Summing the probabilities (0-1) from Softmax and the raw scores (varying range)
        # from OVR is a simple averaging strategy.

        total_scores = softmax_scores + ovr_scores
        model_count = 2 # Hardcoded since we know we have two models

        # Average the scores
        avg_scores = total_scores / model_count
        return avg_scores

    def predict(self, X):
        # Predict class based on the highest average score
        avg_scores = self.predict_scores(X)
        return np.argmax(avg_scores, axis=1).reshape(-1, 1)

print("\n--- Step 8: Ensemble (Softmax + OVR) Implementation ---")

# 1. Instantiate Ensemble Model
ensemble_model = EnsembleClassifier(FINAL_MODELS)

# 2. Evaluate on Full Validation Set
start_time_ensemble_eval = timer_start()
y_pred_val_ensemble = ensemble_model.predict(X_val_norm)
ensemble_eval_time = timer_end(start_time_ensemble_eval, "Ensemble Evaluation")

# 3. Calculate metrics
acc_val_ensemble = accuracy_score(y_val, y_pred_val_ensemble)
metrics_val_ensemble = precision_recall_f1_macro(y_val, y_pred_val_ensemble)

# 4. Tabulate and Store Results
ensemble_final_results = {
    'Model': 'Ensemble (Softmax + OVR)',
    'Train_Time': FINAL_RESULTS[0]['Train_Time'] + FINAL_RESULTS[1]['Train_Time'], # Sum of constituent training times
    'Accuracy': acc_val_ensemble,
    'Macro_F1': metrics_val_ensemble['macro_f1'],
    'Confusion_Matrix': metrics_val_ensemble['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_ensemble
}

FINAL_RESULTS.append(ensemble_final_results)

print("\n" + "="*60)
print("FINAL ENSEMBLE EVALUATION (Softmax + OVR)")
print("="*60)
print(f"Validation Accuracy: {ensemble_final_results['Accuracy']:.4f}")
print(f"Validation Macro F1 Score: {ensemble_final_results['Macro_F1']:.4f}")
print(f"Evaluation Time: {ensemble_eval_time:.4f} seconds")

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': ensemble_final_results['Accuracy'],
     'macro_f1': ensemble_final_results['Macro_F1'],
     'per_class_metrics': ensemble_final_results['Per_Class_Metrics']},
    set_name="Full Validation Set"
)

print("\nProceeding to Final Summary and Analysis (Step 9).")

class KMeansClassifier:

    def __init__(self, k=10, max_iter=30, n_init=1, random_seed=42):
        self.k = k
        self.max_iter = max_iter
        self.n_init = n_init
        self.centers = None
        self.cluster_to_label = {}
        np.random.seed(random_seed) # Set seed for reproducibility

    def _euclidean_distance(self, X, centers):
        """Computes N x K distance matrix."""
        # X is N x D, centers is K x D
        # Using broadcasting for efficient distance calculation: ||X - C||^2 = ||X||^2 + ||C||^2 - 2 * X @ C.T
        X_sq = np.sum(X**2, axis=1, keepdims=True) # N x 1
        C_sq = np.sum(centers**2, axis=1, keepdims=True).T # 1 x K
        XC_prod = X @ centers.T # N x K
        distances = np.sqrt(X_sq + C_sq - 2 * XC_prod)
        return distances

    def _initialize_centers(self, X, k):
        """Randomly select k data points as initial centers."""
        indices = np.random.choice(X.shape[0], k, replace=False)
        return X[indices]

    def _assign_clusters(self, X, centers):
        """Assign each sample to the closest center."""
        distances = self._euclidean_distance(X, centers)
        return np.argmin(distances, axis=1)

    def _update_centers(self, X, labels, k):
        """Recalculate centers as the mean of all points assigned to that cluster."""
        new_centers = np.zeros((k, X.shape[1]))
        for i in range(k):
            points = X[labels == i]
            if len(points) > 0:
                new_centers[i] = np.mean(points, axis=0)
            else:
                # If a cluster is empty, re-initialize it randomly
                new_centers[i] = X[np.random.randint(0, X.shape[0])]
        return new_centers

    def _map_clusters_to_labels(self, y_train, assignments, k):
        """Map each cluster index to the majority true label of its assigned points."""
        mapping = {}
        for i in range(k):
            labels = y_train[assignments == i].flatten()
            if len(labels) > 0:
                # Find the most frequent label in the cluster
                unique_labels, counts = np.unique(labels, return_counts=True)
                mapping[i] = unique_labels[np.argmax(counts)]
            else:
                # Default to class 0 if cluster is empty
                mapping[i] = 0
        return mapping

    def fit(self, X, y):

        best_inertia = np.inf
        start_time = timer_start()
        print(f"--- Starting KMeans Training (k={self.k}, Max Iter={self.max_iter}, N Init={self.n_init}) ---")

        for run in range(self.n_init):
            centers = self._initialize_centers(X, self.k)

            for iteration in range(self.max_iter):
                assignments = self._assign_clusters(X, centers)
                new_centers = self._update_centers(X, assignments, self.k)

                # Check for convergence (centers didn't change significantly)
                if np.allclose(centers, new_centers):
                    break
                centers = new_centers

            # Calculate inertia (Sum of squared distances to closest cluster center)
            distances = self._euclidean_distance(X, centers)
            inertia = np.sum(np.min(distances, axis=1)**2)

            if inertia < best_inertia:
                best_inertia = inertia
                best_centers = centers
                best_assignments = assignments

        self.centers = best_centers
        self.cluster_to_label = self._map_clusters_to_labels(y, best_assignments, self.k)

        timer_end(start_time, "KMeans Training")
        return self.centers, self.cluster_to_label

    def predict(self, X):
        if self.centers is None:
            raise ValueError("Model not fitted.")

        # Assign points to the closest cluster
        assignments = self._assign_clusters(X, self.centers)

        # Map cluster index to the learned label
        y_pred = np.array([self.cluster_to_label[i] for i in assignments]).reshape(-1, 1)
        return y_pred

# --- Initial Test Run (KMeans Tuning) ---
KMEANS_DEFAULTS = {
    'k': 10,
    'max_iter': 20, # Reduced iteration for fast initial test
    'n_init': 2
}

print("\n--- Step 12: k-means Clustering Classifier Implementation ---")
print("Implementation complete. Running initial test on dev subset.")

# We train on X_train_dev (3000 samples)
kmeans_model_test = KMeansClassifier(**KMEANS_DEFAULTS)
kmeans_model_test.fit(X_train_dev, y_train_dev)

# Evaluate on X_val_dev
y_pred_kmeans_dev = kmeans_model_test.predict(X_val_dev)

# Calculate metrics
acc_kmeans_dev = accuracy_score(y_val_dev, y_pred_kmeans_dev)
metrics_kmeans_dev = precision_recall_f1_macro(y_val_dev, y_pred_kmeans_dev)

print("\n--- Initial KMeans Dev Validation Results ---")
print(f"Accuracy: {acc_kmeans_dev:.4f}")
print(f"Macro F1: {metrics_kmeans_dev['macro_f1']:.4f}")

print("\nProceeding to KMeans Hyperparameter Tuning.")

# --- Step 12 Continued: Hyperparameter Tuning for KMeans Classifier ---

# Hyperparameter Grid
K_VALUES = [8, 10, 12]
MAX_ITER_VALUES = [20, 30]
N_INIT_VALUES = [2, 3]

kmeans_tuning_results = []
best_kmeans_f1 = -1.0
best_kmeans_config = None

print("\n--- Starting KMeans Hyperparameter Grid Search ---")
total_configs = len(K_VALUES) * len(MAX_ITER_VALUES) * len(N_INIT_VALUES)
print(f"Total configurations to test: {total_configs}")

# Start overall timer
kmeans_tuning_start_time = timer_start()

for k in K_VALUES:
    for max_iter in MAX_ITER_VALUES:
        for n_init in N_INIT_VALUES:

            print(f"\nTesting: k={k}, Max Iter={max_iter}, N Init={n_init}")

            # 1. Initialize and Train Model
            # Note: We keep the same random seed for all tests to ensure fairness,
            # but the n_init runs within the model itself use internal randomness.
            model = KMeansClassifier(
                k=k,
                max_iter=max_iter,
                n_init=n_init,
                random_seed=42
            )

            # Train on X_train_dev (3000 samples)
            model.fit(X_train_dev, y_train_dev)

            # 2. Evaluate on Train Dev Set
            y_pred_train_dev = model.predict(X_train_dev)
            acc_train_dev = accuracy_score(y_train_dev, y_pred_train_dev)
            f1_train_dev = precision_recall_f1_macro(y_train_dev, y_pred_train_dev)['macro_f1']

            # 3. Evaluate on Validation Dev Set (for selection)
            y_pred_val_dev = model.predict(X_val_dev)
            acc_val_dev = accuracy_score(y_val_dev, y_pred_val_dev)
            f1_val_dev = precision_recall_f1_macro(y_val_dev, y_pred_val_dev)['macro_f1']

            print(f"  Train Dev F1: {f1_train_dev:.4f} | Val Dev F1: {f1_val_dev:.4f}")

            # 4. Store Results
            config_result = {
                'K': k,
                'Max_Iter': max_iter,
                'N_Init': n_init,
                'Train_Acc': acc_train_dev,
                'Train_F1': f1_train_dev,
                'Val_Acc': acc_val_dev,
                'Val_F1': f1_val_dev,
            }
            kmeans_tuning_results.append(config_result)

            # 5. Check for Best Configuration
            if f1_val_dev > best_kmeans_f1:
                best_kmeans_f1 = f1_val_dev
                best_kmeans_config = config_result

timer_end(kmeans_tuning_start_time, "\nTotal KMeans Hyperparameter Tuning")

# Display Best Result
print("\n" + "="*60)
print("BEST K-MEANS CLASSIFIER CONFIGURATION (Based on Val Dev Macro F1 Score):")
print(f"Macro F1 Score: {best_kmeans_config['Val_F1']:.4f}")
print(f"Hyperparameters: K={best_kmeans_config['K']}, Max Iter={best_kmeans_config['Max_Iter']}, N Init={best_kmeans_config['N_Init']}")
print("="*60)

# Save the full results table to a CSV for later analysis/reporting
kmeans_tuning_df = pd.DataFrame(kmeans_tuning_results)
kmeans_tuning_df.to_csv("kmeans_tuning_results.csv", index=False)
print("Full tuning results saved to 'kmeans_tuning_results.csv'")

print("\nProceeding to Final KMeans training & evaluation.")

# --- Step 13: Final KMeans training & evaluation ---

BEST_KMEANS_CONFIG = {
    'k': 12,
    'max_iter': 30,
    'n_init': 2
}

print("\n--- Training Final KMeans Model on Full Data ---")
print(f"Hyperparameters: K={BEST_KMEANS_CONFIG['k']}, Max Iter={BEST_KMEANS_CONFIG['max_iter']}, N Init={BEST_KMEANS_CONFIG['n_init']}")

# 1. Initialize and Train Model on full training data
final_kmeans_model = KMeansClassifier(**BEST_KMEANS_CONFIG)

# Record training time separately
start_time_full_train = timer_start()
# X_train_norm contains the pixel data, y_train contains the true labels
final_kmeans_model.fit(X_train_norm, y_train)
kmeans_train_time = timer_end(start_time_full_train, "Final KMeans Training")


# 2. Evaluate on Full Validation Set
y_pred_val_kmeans = final_kmeans_model.predict(X_val_norm)

# Calculate metrics
acc_val_kmeans = accuracy_score(y_val, y_pred_val_kmeans)
metrics_val_kmeans = precision_recall_f1_macro(y_val, y_pred_val_kmeans)

# 3. Tabulate and Store Results
kmeans_final_results = {
    'Model': 'KMeans Classifier',
    'Train_Time': kmeans_train_time,
    'Accuracy': acc_val_kmeans,
    'Macro_F1': metrics_val_kmeans['macro_f1'],
    'Confusion_Matrix': metrics_val_kmeans['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_kmeans
}

# Add results to the existing list (FINAL_RESULTS)
FINAL_MODELS['KMeans'] = final_kmeans_model
FINAL_RESULTS.append(kmeans_final_results)

print("\n" + "="*60)
print("FINAL KMEANS CLASSIFIER EVALUATION (Full Validation Set)")
print("="*60)
print(f"Validation Accuracy: {kmeans_final_results['Accuracy']:.4f}")
print(f"Validation Macro F1 Score: {kmeans_final_results['Macro_F1']:.4f}")
print(f"Training Time: {kmeans_final_results['Train_Time']:.4f} seconds")

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': kmeans_final_results['Accuracy'],
     'macro_f1': kmeans_final_results['Macro_F1'],
     'per_class_metrics': kmeans_final_results['Per_Class_Metrics']},
    set_name="Full Validation Set"
)

print("\nProceeding to Gaussian Anomaly Detection Model Implementation and Tuning.")

# --- Step 14: Gaussian Classifier Implementation ---

class GaussianClassifier:

    def __init__(self, epsilon=1e-3):
        self.epsilon = epsilon  # Variance smoothing parameter
        self.means = {}         # Dictionary to store mean vectors per class
        self.variances = {}     # Dictionary to store variance vectors per class
        self.n_classes = 10     # Fixed for MNIST

    def fit(self, X, y):
        # Flatten y for easier indexing
        y_flat = y.flatten()

        # Calculate means and variances for each class
        for c in range(self.n_classes):
            X_c = X[y_flat == c]

            # Calculate mean vector (1 x D)
            self.means[c] = np.mean(X_c, axis=0)

            # Calculate variance vector (1 x D)
            self.variances[c] = np.var(X_c, axis=0)

            # Add smoothing epsilon to variances
            self.variances[c] += self.epsilon

    def predict_scores(self, X):
        N, D = X.shape
        log_likelihoods = np.zeros((N, self.n_classes))

        # Constant offset term -D/2 * log(2pi) is ignored as it cancels out during argmax

        for c in range(self.n_classes):
            mu_c = self.means[c]
            sigma2_c = self.variances[c]

            # 1. Log-determinant term: -1/2 * sum(log(sigma^2))
            log_det_term = -0.5 * np.sum(np.log(sigma2_c))

            # 2. Mahalanobis distance term: -1/2 * sum((x - mu)^2 / sigma^2)
            # (X - mu_c) is N x D
            diff = X - mu_c
            mahalanobis_term = -0.5 * np.sum(diff**2 / sigma2_c, axis=1) # N x 1

            log_likelihoods[:, c] = log_det_term + mahalanobis_term

        return log_likelihoods

    def predict(self, X):
        log_likelihoods = self.predict_scores(X)
        # Predict class with the highest log-likelihood
        return np.argmax(log_likelihoods, axis=1).reshape(-1, 1)

# --- Initial Test Run (Gaussian Tuning) ---
GAUSSIAN_DEFAULTS = {
    'epsilon': 1e-3
}

print("\n--- Step 14: Gaussian Classifier Implementation ---")
print("Implementation complete. Running initial test on dev subset.")

# 1. Initialize and Train Model
start_time_test = timer_start()
gaussian_model_test = GaussianClassifier(**GAUSSIAN_DEFAULTS)

# Training is just calculating means and variances on X_train_dev
gaussian_model_test.fit(X_train_dev, y_train_dev)
timer_end(start_time_test, "Gaussian Training (Dev)")

# 2. Evaluate on X_val_dev
y_pred_gaussian_dev = gaussian_model_test.predict(X_val_dev)

# Calculate metrics
acc_gaussian_dev = accuracy_score(y_val_dev, y_pred_gaussian_dev)
metrics_gaussian_dev = precision_recall_f1_macro(y_val_dev, y_pred_gaussian_dev)

print("\n--- Initial Gaussian Dev Validation Results ---")
print(f"Accuracy: {acc_gaussian_dev:.4f}")
print(f"Macro F1: {metrics_gaussian_dev['macro_f1']:.4f}")

print("\nProceeding to Gaussian Hyperparameter Tuning.")

# --- Step 15: Final Gaussian training & evaluation ---

BEST_GAUSSIAN_CONFIG = {
    'epsilon': 1e-4
}

print("\n--- Training Final Gaussian Model on Full Data ---")
print(f"Hyperparameters: Epsilon={BEST_GAUSSIAN_CONFIG['epsilon']}")

# 1. Initialize and Train Model on full training data
final_gaussian_model = GaussianClassifier(**BEST_GAUSSIAN_CONFIG)

# Record training time
start_time_full_train = timer_start()
# Training means calculating means and variances on X_train_norm
final_gaussian_model.fit(X_train_norm, y_train)
gaussian_train_time = timer_end(start_time_full_train, "Final Gaussian Training")


# 2. Evaluate on Full Validation Set
y_pred_val_gaussian = final_gaussian_model.predict(X_val_norm)

# Calculate metrics
acc_val_gaussian = accuracy_score(y_val, y_pred_val_gaussian)
metrics_val_gaussian = precision_recall_f1_macro(y_val, y_pred_val_gaussian)

# 3. Tabulate and Store Results
gaussian_final_results = {
    'Model': 'Gaussian Classifier',
    'Train_Time': gaussian_train_time,
    'Accuracy': acc_val_gaussian,
    'Macro_F1': metrics_val_gaussian['macro_f1'],
    'Confusion_Matrix': metrics_val_gaussian['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_gaussian
}

# Add results and model
FINAL_MODELS['Gaussian'] = final_gaussian_model
FINAL_RESULTS.append(gaussian_final_results)

print("\n" + "="*60)
print("FINAL GAUSSIAN CLASSIFIER EVALUATION (Full Validation Set)")
print("="*60)
print(f"Validation Accuracy: {gaussian_final_results['Accuracy']:.4f}")
print(f"Validation Macro F1 Score: {gaussian_final_results['Macro_F1']:.4f}")
print(f"Training Time: {gaussian_final_results['Train_Time']:.4f} seconds")

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': gaussian_final_results['Accuracy'],
     'macro_f1': gaussian_final_results['Macro_F1'],
     'per_class_metrics': gaussian_final_results['Per_Class_Metrics']},
    set_name="Full Validation Set"
)

print("\nProceeding to Ensemble Classifier Implementation.")

class EnsembleClassifier:
    """Ensembles multiple classifiers by averaging their Softmax-converted output scores."""

    def __init__(self, models):
        self.models = models
        # Initialize with equal weights
        self.weights = {'Softmax': 1.0, 'OVR': 1.0, 'KMeans': 1.0, 'Gaussian': 1.0}
        self.model_names = list(models.keys())

    def _softmax(self, z):
        """Standard softmax function for converting scores/logits to probabilities."""
        if z.size == 0 or not np.issubdtype(z.dtype, np.number):
             return np.zeros((z.shape[0], 10))

        # Subtract max for numerical stability
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        sum_exp = np.sum(exp_z, axis=1, keepdims=True)
        # Avoid division by zero
        sum_exp[sum_exp == 0] = 1
        return exp_z / sum_exp

    def predict_scores(self, X):

        N = X.shape[0]
        K = 10
        total_weighted_scores = np.zeros((N, K))

        # 1. Softmax (Logistic Regression) - Already Probabilities
        softmax_scores = self.models['Softmax'].predict_proba(X)
        total_weighted_scores += self.weights['Softmax'] * softmax_scores

        # 2. OVR (Linear Regression) - Raw Scores -> Softmax
        ovr_raw_scores = self.models['OVR'].predict_scores(X)
        ovr_scores = self._softmax(ovr_raw_scores)
        total_weighted_scores += self.weights['OVR'] * ovr_scores

        # 3. Gaussian (Log-Likelihoods) - Log-Likelihoods -> Softmax
        gaussian_log_scores = self.models['Gaussian'].predict_scores(X)
        gaussian_scores = self._softmax(gaussian_log_scores)
        total_weighted_scores += self.weights['Gaussian'] * gaussian_scores

        # 4. KMeans (Hard Labels) - Hard Labels -> One-Hot Scores
        kmeans_hard_preds = self.models['KMeans'].predict(X).flatten()
        kmeans_scores = np.zeros((N, K))
        kmeans_scores[np.arange(N), kmeans_hard_preds] = 1.0
        total_weighted_scores += self.weights['KMeans'] * kmeans_scores

        # Normalize by the sum of weights
        total_weight = sum(self.weights.values())
        avg_scores = total_weighted_scores / total_weight
        return avg_scores

    def predict(self, X):
        avg_scores = self.predict_scores(X)
        return np.argmax(avg_scores, axis=1).reshape(-1, 1)

    # The method that was missing during the execution
    def set_weights(self, weights):
        """Sets new weights for the models."""
        new_weights = {}
        for name in self.model_names:
            if name not in weights:
                raise ValueError(f"Weight missing for model: {name}")
            new_weights[name] = weights[name]
        self.weights = new_weights

# --- Step 18: Final Ensemble Training & Evaluation ---

# Best weights determined from tuning
BEST_ENSEMBLE_WEIGHTS = {
    'Softmax': 5,
    'OVR': 1,
    'KMeans': 0,
    'Gaussian': 1
}

print("\n--- Training Final Ensemble Model on Full Data ---")
print(f"Weights: {BEST_ENSEMBLE_WEIGHTS}")

# 1. Initialize Ensemble Model
# NOTE: The ensemble is "trained" by simply setting the weights,
# as the base models (in FINAL_MODELS) are already fully trained on the full dataset.
final_ensemble_model = EnsembleClassifier(FINAL_MODELS)
final_ensemble_model.set_weights(BEST_ENSEMBLE_WEIGHTS)

# Training time is negligible (just setting weights)
ensemble_train_time = 0.0

# 2. Evaluate on Full Validation Set
start_time_full_eval = timer_start()
y_pred_val_ensemble = final_ensemble_model.predict(X_val_norm)
ensemble_eval_time = timer_end(start_time_full_eval, "Final Ensemble Evaluation")

# Calculate metrics
acc_val_ensemble = accuracy_score(y_val, y_pred_val_ensemble)
metrics_val_ensemble = precision_recall_f1_macro(y_val, y_pred_val_ensemble)

# 3. Tabulate and Store Results
ensemble_final_results = {
    'Model': 'Ensemble (Weighted Soft Voting)',
    'Train_Time': ensemble_train_time,
    'Accuracy': acc_val_ensemble,
    'Macro_F1': metrics_val_ensemble['macro_f1'],
    'Confusion_Matrix': metrics_val_ensemble['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_ensemble
}

# Add results
FINAL_RESULTS.append(ensemble_final_results)

print("\n" + "="*60)
print("FINAL ENSEMBLE CLASSIFIER EVALUATION (Full Validation Set)")
print("="*60)
print(f"Validation Accuracy: {ensemble_final_results['Accuracy']:.4f}")
print(f"Validation Macro F1 Score: {ensemble_final_results['Macro_F1']:.4f}")
print(f"Training Time (Base Models): {sum(r['Train_Time'] for r in FINAL_RESULTS[:-1]):.4f} seconds")

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': ensemble_final_results['Accuracy'],
     'macro_f1': ensemble_final_results['Macro_F1'],
     'per_class_metrics': ensemble_final_results['Per_Class_Metrics']},
    set_name="Full Validation Set"
)

print("\nAll model training and evaluation steps are complete. Proceeding to final deliverable preparation.")

# --- Step 19: Stacking Ensemble Implementation ---

print("\n--- Generating Meta-Features (Layer 0) on Full Training Data ---")

# Assuming FINAL_MODELS contains the trained Softmax, OVR, KMeans, and Gaussian models.
N_train = X_train_norm.shape[0]
N_val = X_val_norm.shape[0]
K = 10 # 10 classes

# --- Layer 0 Feature Generation ---
# The number of meta-features will be 4 models * 10 classes = 40 features
X_meta_train = np.zeros((N_train, 4 * K))
X_meta_val = np.zeros((N_val, 4 * K))

# 1. Softmax (Probabilities)
X_meta_train[:, 0:10] = FINAL_MODELS['Softmax'].predict_proba(X_train_norm)
X_meta_val[:, 0:10] = FINAL_MODELS['Softmax'].predict_proba(X_val_norm)

# 2. OVR (Softmax-converted Scores)
# Use the _softmax helper function from the EnsembleClassifier (assuming it's available)
ovr_train_scores = FINAL_MODELS['OVR'].predict_scores(X_train_norm)
X_meta_train[:, 10:20] = EnsembleClassifier._softmax(None, ovr_train_scores)
ovr_val_scores = FINAL_MODELS['OVR'].predict_scores(X_val_norm)
X_meta_val[:, 10:20] = EnsembleClassifier._softmax(None, ovr_val_scores)

# 3. Gaussian (Softmax-converted Log-Likelihoods)
gaussian_train_log_scores = FINAL_MODELS['Gaussian'].predict_scores(X_train_norm)
X_meta_train[:, 20:30] = EnsembleClassifier._softmax(None, gaussian_train_log_scores)
gaussian_val_log_scores = FINAL_MODELS['Gaussian'].predict_scores(X_val_norm)
X_meta_val[:, 20:30] = EnsembleClassifier._softmax(None, gaussian_val_log_scores)

# 4. KMeans (Hard Prediction -> One-Hot)
kmeans_train_preds = FINAL_MODELS['KMeans'].predict(X_train_norm).flatten()
kmeans_train_one_hot = np.zeros((N_train, K))
kmeans_train_one_hot[np.arange(N_train), kmeans_train_preds] = 1.0
X_meta_train[:, 30:40] = kmeans_train_one_hot

kmeans_val_preds = FINAL_MODELS['KMeans'].predict(X_val_norm).flatten()
kmeans_val_one_hot = np.zeros((N_val, K))
kmeans_val_one_hot[np.arange(N_val), kmeans_val_preds] = 1.0
X_meta_val[:, 30:40] = kmeans_val_one_hot


# --- Layer 1 Meta-Model Training (Using Softmax Classifier) ---
print("\n--- Training Softmax Meta-Classifier (Layer 1) ---")

# Use the same optimal hyperparameters from the original Softmax tuning
META_MODEL_CONFIG = {
    'learning_rate': 1e-2,  # alpha
    'lambda_reg': 1e-5,     # lambda
    'max_iterations': 500
}

meta_model = SoftmaxClassifier(
    learning_rate=META_MODEL_CONFIG['learning_rate'],
    lambda_reg=META_MODEL_CONFIG['lambda_reg']
)

# Train the Meta-Model on the new features (X_meta_train) and original labels (y_train)
start_time_meta_train = timer_start()
meta_model.fit(X_meta_train, y_train, max_iterations=META_MODEL_CONFIG['max_iterations'])
meta_train_time = timer_end(start_time_meta_train, "Meta-Model Training")

# --- Final Evaluation of Stacking Ensemble ---
print("\n--- FINAL STACKING ENSEMBLE EVALUATION (Full Validation Set) ---")

y_pred_val_stacking = meta_model.predict(X_meta_val)
acc_val_stacking = accuracy_score(y_val, y_pred_val_stacking)
metrics_val_stacking = precision_recall_f1_macro(y_val, y_pred_val_stacking)

print("\n" + "="*60)
print(f"Validation Accuracy: {acc_val_stacking:.4f}")
print(f"Validation Macro F1 Score: {metrics_val_stacking['macro_f1']:.4f}")
print(f"Meta-Model Training Time: {meta_train_time:.4f} seconds")
print("="*60)

# Helper print for per-class metrics
print_metrics_table(
    {'accuracy': acc_val_stacking,
     'macro_f1': metrics_val_stacking['macro_f1'],
     'per_class_metrics': metrics_val_stacking},
    set_name="Full Validation Set (Stacking)"
)

FINAL_RESULTS.append({
    'Model': 'Stacking Ensemble',
    'Train_Time': meta_train_time,
    'Accuracy': acc_val_stacking,
    'Macro_F1': metrics_val_stacking['macro_f1'],
    'Confusion_Matrix': metrics_val_stacking['confusion_matrix'],
    'Per_Class_Metrics': metrics_val_stacking
})

print("\nProceeding to final time and performance discussion.")