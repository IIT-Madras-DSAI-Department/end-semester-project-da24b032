# -*- coding: utf-8 -*-
"""algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhoyAkwq_yCB_Ipu3ZUcvNModU9zV5CJ
"""

import numpy as np
from collections import defaultdict
import random

# Helper 1: Converts a label vector y (N,) to a one-hot matrix (N, K).
def _one_hot(y, K):
    """Converts a label vector y (N,) to a one-hot matrix (N, K)."""
    N = y.shape[0]
    Y_one_hot = np.zeros((N, K))
    indices = y.flatten().astype(int)
    Y_one_hot[np.arange(N), indices] = 1
    return Y_one_hot

# Helper 2: Computes the stable softmax activation (for predict_proba).
def _stable_softmax(z):
    """Computes the softmax activation for numerical stability."""
    # Subtract max for numerical stability
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

class SoftmaxClassifier:
    """Multiclass Logistic Regression using Gradient Descent."""
    def __init__(self, learning_rate=1e-2, lambda_reg=1e-5, max_iterations=500):
        self.learning_rate = learning_rate
        self.lambda_reg = lambda_reg
        self.max_iterations = max_iterations
        self.W = None

    def fit(self, X, y):
        N, D = X.shape
        K = 10

        X_b = np.hstack([np.ones((N, 1)), X])
        Y_one_hot = _one_hot(y, K)

        if self.W is None:
            self.W = 0.01 * np.random.randn(D + 1, K)

        for i in range(self.max_iterations):
            S = X_b @ self.W
            P = _stable_softmax(S)

            gradient = (X_b.T @ (P - Y_one_hot)) / N

            # Add L2 regularization gradient (excluding the bias term W[0,:])
            reg_gradient = self.lambda_reg * self.W
            reg_gradient[0, :] = 0
            gradient += reg_gradient

            self.W -= self.learning_rate * gradient

    def predict_proba(self, X):
        N = X.shape[0]
        X_b = np.hstack([np.ones((N, 1)), X])
        S = X_b @ self.W
        return _stable_softmax(S)

    def predict(self, X):
        probabilities = self.predict_proba(X)
        return np.argmax(probabilities, axis=1).reshape(-1, 1)

class OVRClassifier:
    """One-vs-Rest Linear Regression (using MSE loss) with Gradient Descent."""
    def __init__(self, learning_rate=1e-2, lambda_reg=1e-5, max_iterations=500):
        self.learning_rate = learning_rate
        self.lambda_reg = lambda_reg
        self.max_iterations = max_iterations
        self.W = None

    def fit(self, X, y):
        N, D = X.shape
        K = 10

        X_b = np.hstack([np.ones((N, 1)), X])
        Y_one_hot = _one_hot(y, K)

        if self.W is None:
            self.W = 0.01 * np.random.randn(D + 1, K)

        for i in range(self.max_iterations):
            S = X_b @ self.W
            E = S - Y_one_hot

            # Compute gradient for MSE
            gradient = (X_b.T @ E) / N

            # Add L2 regularization gradient
            reg_gradient = self.lambda_reg * self.W
            reg_gradient[0, :] = 0
            gradient += reg_gradient

            self.W -= self.learning_rate * gradient

    def predict_scores(self, X):
        """Returns the raw linear scores for each class."""
        N = X.shape[0]
        X_b = np.hstack([np.ones((N, 1)), X])
        return X_b @ self.W

    def predict_proba(self, X):
        """Returns softmax-converted scores for ensemble compatibility."""
        scores = self.predict_scores(X)
        return _stable_softmax(scores)

    def predict(self, X):
        scores = self.predict_scores(X)
        return np.argmax(scores, axis=1).reshape(-1, 1)

class KMeansClassifier:
    """K-Means Clustering with classification via majority voting on centroids."""
    def __init__(self, n_clusters=12, max_iterations=100, random_state=42):
        self.K = n_clusters
        self.max_iterations = max_iterations
        self.random_state = random_state
        self.centroids = None
        self.y_map = None # Map from centroid index to MNIST label

    def _euclidean_distance(self, X, centroids):
        """Computes the squared Euclidean distance."""
        X_sq = np.sum(X**2, axis=1, keepdims=True)
        C_sq = np.sum(centroids**2, axis=1)
        dot_product = -2 * (X @ centroids.T)
        distances = dot_product + X_sq + C_sq
        return distances

    def fit(self, X, y):
        N, D = X.shape
        np.random.seed(self.random_state)

        # Initialize centroids (using random sampling)
        indices = np.random.choice(N, self.K, replace=False)
        self.centroids = X[indices]

        for i in range(self.max_iterations):
            # Assignment step
            distances = self._euclidean_distance(X, self.centroids)
            labels = np.argmin(distances, axis=1)

            # Update step
            new_centroids = np.zeros_like(self.centroids)
            for k in range(self.K):
                cluster_points = X[labels == k]
                if cluster_points.shape[0] > 0:
                    new_centroids[k] = np.mean(cluster_points, axis=0)

            if np.allclose(self.centroids, new_centroids):
                break

            self.centroids = new_centroids

        # Build class mapping (Centroid index -> MNIST label)
        self.y_map = np.zeros(self.K, dtype=int)
        for k in range(self.K):
            cluster_labels = y[labels == k].flatten()
            if cluster_labels.size > 0:
                # Assign the most frequent true label
                self.y_map[k] = np.argmax(np.bincount(cluster_labels))
            else:
                self.y_map[k] = np.random.randint(0, 10)

    def predict(self, X):
        distances = self._euclidean_distance(X, self.centroids)
        nearest_centroid_index = np.argmin(distances, axis=1)
        return self.y_map[nearest_centroid_index].reshape(-1, 1)

    def predict_proba(self, X):
        """Returns one-hot predictions for ensemble/stacking compatibility."""
        preds = self.predict(X)
        return _one_hot(preds, 10)

class GaussianClassifier:
    """Gaussian Generative Model (GDA) using log-likelihood."""
    def __init__(self, epsilon=1e-4):
        self.epsilon = epsilon # Variance smoothing
        self.means = None
        self.covs = None
        self.priors = None

    def fit(self, X, y):
        N, D = X.shape
        K = 10
        self.means = np.zeros((K, D))
        self.covs = []
        self.priors = np.zeros(K)
        y_flat = y.flatten()

        for k in range(K):
            X_k = X[y_flat == k]
            N_k = X_k.shape[0]

            self.priors[k] = N_k / N
            self.means[k] = np.mean(X_k, axis=0)

            # Covariance matrix
            cov_k = (X_k - self.means[k]).T @ (X_k - self.means[k]) / N_k

            # Add regularization/smoothing (epsilon * I)
            cov_k += np.eye(D) * self.epsilon
            self.covs.append(cov_k)

    def predict_scores(self, X):
        """Returns the unnormalized log-likelihood scores."""
        N, D = X.shape
        K = 10
        log_scores = np.zeros((N, K))

        for k in range(K):
            if self.priors[k] == 0: continue

            cov_k = self.covs[k]
            det_k = np.linalg.det(cov_k)

            if det_k < 1e-15: # Handle near-singular matrix
                log_scores[:, k] = -1e6
                continue

            inv_cov_k = np.linalg.inv(cov_k)
            X_centered = X - self.means[k]

            # Mahalanobis distance squared
            mahalanobis_sq = np.sum((X_centered @ inv_cov_k) * X_centered, axis=1)

            # Log-likelihood calculation
            log_likelihood = -0.5 * D * np.log(2 * np.pi) - 0.5 * np.log(det_k) - 0.5 * mahalanobis_sq

            # Total log score: log(P(x|y=k)) + log(P(y=k))
            log_scores[:, k] = log_likelihood + np.log(self.priors[k])

        return log_scores

    def predict_proba(self, X):
        """Returns softmax-converted log-likelihoods for ensemble compatibility."""
        scores = self.predict_scores(X)
        return _stable_softmax(scores)

    def predict(self, X):
        scores = self.predict_scores(X)
        return np.argmax(scores, axis=1).reshape(-1, 1)

class EnsembleClassifier:
    """
    Handles both Weighted Soft Voting and feature extraction for Stacking.
    """
    def __init__(self, models):
        self.models = models
        self.weights = None

    def set_weights(self, weights_dict):
        """Sets the weights for weighted soft voting."""
        self.weights = weights_dict

    def _get_scores(self, X):
        """
        Collects probability/score matrices from all base models.
        (Used for both soft voting and feature extraction)
        """
        scores = {}
        for name, model in self.models.items():
            # All model classes are designed to have a predict_proba method
            # that returns a stable, 10-column score/probability matrix.
            scores[name] = model.predict_proba(X)
        return scores

    def predict(self, X):
        """
        Performs prediction using the stored weights (for Weighted Soft Voting).
        """
        if self.weights is None:
            raise ValueError("Weights must be set for soft voting ensemble.")

        N = X.shape[0]
        K = 10
        final_scores = np.zeros((N, K))

        raw_scores = self._get_scores(X)

        for name, weight in self.weights.items():
            if weight > 0 and name in raw_scores:
                final_scores += raw_scores[name] * weight

        return np.argmax(final_scores, axis=1).reshape(-1, 1)